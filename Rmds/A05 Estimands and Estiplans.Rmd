---
title: "A05  Estimands and Estiplans"
author: "David Mendez"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Book

## 5.1. Spurious association

```{r}
library(rethinking)
data("WaffleDivorce")
d <- WaffleDivorce

d$D <- standardize(d$Divorce)
d$M <- standardize(d$Marriage)
d$A <- standardize(d$MedianAgeMarriage)
```

```{r}
m5.1 <- quap(
        alist(
                D  ~ dnorm(mu, sigma),
                mu <- a+bA*A,
                a  ~ dnorm(0,0.2),
                bA ~ dnorm(0,0.5),
                sigma ~dexp(1)
                ), data = d
        )
precis(m5.1)
```

```{r}
set.seed(3101)
prior <- extract.prior(m5.1)
mu <- link (m5.1, post=prior, data=list(A=c(-2,2)))
plot(NULL, xlim=c(-2,2), ylim=c(-2,2))
for (i in 1:150) lines(c(-2,2), mu[i,], col=col.alpha("black",0.4))
```

```{r}
A_seq <- seq(from=-3, to=3.2, length.out=30)
mu <- link(m5.1, data=list(A=A_seq))
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(D ~A, data=d, col=rangi2)
lines(A_seq, mu.mean, lwd=2)
shade(mu.PI, A_seq)
```

```{r}
m5.2 <- quap(
        alist(
                D  ~ dnorm(mu, sigma),
                mu <- a+bM*M,
                a  ~ dnorm(0,0.2),
                bM ~ dnorm(0,0.5),
                sigma ~dexp(1)
                ), data = d
        )
M_seq <- seq(from=-3, to=3.2, length.out=30)
mu <- link(m5.2, data=list(M=M_seq))
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(D ~M, data=d, col=rangi2)
lines(M_seq, mu.mean, lwd=2)
shade(mu.PI, M_seq)
```

```{r}
library(dagitty)
DMA_dag2 <- dagitty('dag{D <- A -> M }')
impliedConditionalIndependencies(DMA_dag2)
```

### 5.1.3. Multiple regression notation

```{r}
m5.3 <- quap(
        alist(
                D  ~ dnorm(mu, sigma),
                mu <- a+bM*M+bA*A,
                a  ~ dnorm(0,0.2),
                bA ~ dnorm(0,0.5),
                bM ~ dnorm(0,0.5),
                sigma ~dexp(1)
                ), data = d
        )
plot(coeftab(m5.1, m5.2, m5.3), par=c("bA","bM"))
```

### 5.1.5. Plotting multivariate posteriors

```{r}
m5.4 <- quap(
        alist(
              D  ~ dnorm(mu, sigma),
              mu <- a+bAM*A,
              a  ~ dnorm(0,0.2),
              bAM ~ dnorm(0,0.5),
              sigma ~dexp(1)
              ), data = d  
        )
mu <- link(m5.4)
mu_mean <- apply(mu, 2, mean)
mu_resid <- d$M - mu_mean
```

#### 5.1.5.2. Posterior prediction plots

```{r}
mu <- link(m5.3)
mu_mean <- apply(mu, 2, mean)
mu_PI <- apply(mu, 2, PI)
D_sim <- sim(m5.3, n=1e4)
D_PI <- apply(D_sim, 2, PI)
plot(mu_mean ~ d$D, col=rangi2, ylim=range(mu_PI),
     xlab="Observed Divorce", ylab="Predicted Divorce")
abline(a=0, b=1, lty=2)
for (i in 1:nrow(d)) lines(rep(d$D[i],2), mu_PI[,i], col=rangi2)
identify(x=d$D, y=mu_mean, labels = d$Loc)
```

```{r}
m5.3_a <- quap(
        alist(
                ## A -> D <- M
              D  ~ dnorm(mu, sigma),
              mu <- a+bA*A+bM*M,
              a  ~ dnorm(0,0.2),
              bM ~ dnorm(0,0.5),
              bA ~ dnorm(0,0.5),
              sigma ~dexp(1),
              ## A->M
              M  ~ dnorm(mu_M, sigma_M),
              mu_M <- aM+bAM*A,
              aM  ~ dnorm(0,0.2),
              bAM ~ dnorm(0,0.5),
              sigma_M ~dexp(1)
              ), data = d  
        )
#precis(m5.3_a)
A_seq <- seq(from=-2, to=2, length.out=30)
sim_dat <- data.frame(A=A_seq)
s <- sim(m5.3_a, data=sim_dat, vars=c("M","D"))
plot(sim_dat$A, colMeans(s$D), ylim=c(-2,2),type="l",xlab="Manipulated A",
     ylab="Counterfactual D")
shade(apply(s$D, 2,PI),sim_dat$A)
mtext("Total Conterfactural effect of A on D")
```

```{r}
sim2_dat <- data.frame(A=(c(20,30)-26.1)/1.24)
s2 <- sim(m5.3_a, data=sim2_dat, vars=c("M","D"))
mean(s2$D[,2]-s2$D[,1])
```

```{r}
sim_dat <- data.frame(M=seq(from=-2,to=2, length.out=30),A=0)
s <- sim(m5.3_a, data=sim_dat, vars="D")
plot(sim_dat$M, colMeans(s), ylim=c(-2,2),type="l",xlab="Manipulated M",
     ylab="Counterfactual D")
shade(apply(s, 2,PI),sim_dat$M)
mtext("Total Conterfactural effect of M on D")
```

## 5.2. Masked Relationship

```{r}
library(rethinking)
data(milk)
d <- milk
str(d)
```

```{r}
d$K <- standardize(d$kcal.per.g)
d$N <- standardize(d$neocortex.perc)
d$M <- standardize(log(d$mass))
```

```{r}
dcc <- d[complete.cases(d$K, d$N, d$M),]
m5.5 <- quap(
        alist(
        K ~ dnorm(mu, sigma),
        mu <- a +bN*N, 
        a ~ dnorm(0,0.2),
        bN ~ dnorm(0,0.5),
        sigma ~ dexp(1)
        ), data=dcc
)
prior <- extract.prior(m5.5)
xseq <- c(-2,2)
mu <- link(m5.5, post=prior,data = list(N=xseq))
plot(NULL, xlim=xseq,ylim=xseq)
for (i in 1:50) lines(xseq, mu[i,], col=col.alpha("black",0.6))
```

```{r}
precis(m5.5)
xseq <- seq(from=min(dcc$N)-0.15,to=max(dcc$N)+0.15,length.out=30)
mu <- link(m5.5, data=list(N=xseq))
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot(K ~N , data=dcc)
lines(xseq, mu_mean, lwd=2)
shade(mu_PI, xseq)
```

```{r}
m5.6 <- quap(
        alist(
        K ~ dnorm(mu, sigma),
        mu <- a +bM*M, 
        a ~ dnorm(0,0.2),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp(1)
        ), data=dcc
)
precis(m5.6)
```

```{r}
m5.7 <- quap(
        alist(
        K ~ dnorm(mu, sigma),
        mu <- a +bM*M+bN*N, 
        a ~ dnorm(0,0.2),
        bN ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp(1)
        ), data=dcc
)
precis(m5.7)
plot(coeftab(m5.5,m5.6,m5.7), pars=c("bM","bN"))
```

```{r}
xseq <- seq(from=min(dcc$N)-0.15,to=max(dcc$N)+0.15,length.out=30)
mu <- link(m5.7, data=data.frame(M=xseq,N=0))
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot(NULL, xlim=range(dcc$M),ylim=range(dcc$K))
lines(xseq, mu_mean, lwd=2)
shade(mu_PI, xseq)
```

## 5.3. Categorical variables

### 5.3.1. Binary categories

```{r}
data("Howell1")
d <- Howell1
```

```{r}
mu_female <- rnorm(1e4, 178, 20)
mu_male <- rnorm(1e4, 178,20)+rnorm(1e4,0,10)
precis(data.frame(mu_female, mu_male))
```

```{r}
d$sex <- d$male+1
m5.8 <- quap(
        alist(
        height~dnorm(mu,sigma),
        mu <- a[sex],
        a[sex]~dnorm(178,20),
        sigma ~dunif(0,50)
),data=d)
precis(m5.8, depth = 2)
```

```{r}
post <- extract.samples(m5.8)
post$diff_fm <- post$a[,1]-post$a[,2]
precis(post,depth=2)
```

### 5.3.2. Many categories

```{r}
data(milk)
d <- milk
levels(d$clade)
d$clade_id <- as.integer(d$clade)
```

```{r}
d$K <- standardize(d$kcal.per.g)
m5.9 <- quap(alist(
        K ~dnorm(mu, sigma),
        mu <- a[clade_id],
        a[clade_id] <- dnorm(0,0.5),
        sigma <- dexp(1)
),data=d)
labels <- paste("a[",1:4,"]:",levels(d$clade),sep="")
plot(precis(m5.9, depth = 2,pars="a"),labels=labels)
```

```{r}
set.seed(63)
d$house <- sample(rep(1:4,each=8), size=nrow(d))
m5.10 <- quap(alist(
        K ~dnorm(mu, sigma),
        mu <- a[clade_id] +h[house],
        a[clade_id] ~ dnorm(0,0.5),
        h[house] ~ dnorm(0,0.5),
        sigma ~ dexp(1)
),data=d)
labels_h <- paste("h[",1:4,"]:",levels(d$house),sep="")
plot(precis(m5.10, depth = 2,pars=c("a","h")),labels=c(labels,labels_h))
```

# Exercises

## Easy

### 5E1. v

Which of the linear models below are multiple linear regressions?

(1) μi = α + β\*xi

(2) μi = βx\*xi + βz\*zi

(3) μi = α + β(xi − zi)

(4) μi = α + βx\**xi + βz*\*zi

    ```         
    2 and 4  have different parameters for each of the variables
    ```

### 5E2. v

Write down a multiple regression to evaluate the claim: Animal diversity is linearly related to latitude, but only after controlling for plant diversity. You just need to write down the model definition.

```         
L->PD->AD
A ~ Normal (mu, sigma)
mu_i = α + β_pd*PD + α + β_l*L
```

### 5E3. v

Write down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree. Write down the model definition and indicate which side of zero each slope parameter should be on.

```         
T ~ Normal (mu, sigma)
mu = α + β_f*F+β_s*S)
```

### 5E4. v

Suppose you have a single categorical predictor with 4 levels (unique values), labeled A, B, C and D. Let Ai be an indicator variable that is 1 where case i is in category A. Also suppose Bi, Ci, and Di for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when it’s possible to compute one posterior distribution from the posterior distribution of another model.

(1) μi = α + βA\*Ai + βB\*Bi + βD\*Di

(2) μi = α + βA\*Ai + βB\*Bi + βC\*Ci + βD\*Di

(3) μi = α + βB\*Bi + βC\*Ci + βD\*Di

(4) μi = αA\*Ai + αB\*Bi + αC\*Ci + αD\*Di

(5) μi = αA\*(1 − Bi − Ci − Di) + αB\*Bi + αC\*Ci + αD\*Di

```         
2 is different than the rest. The rest have an intercept and 3 slopes or 4 slopes while 2 has an intercept and 4 slopes
```

## Medium

### 5M1. v

Invent your own example of a spurious correlation. An outcome variable should be correlated with both predictor variables. But when both predictors are entered in the same model, the correlation between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).

```{r}
library(tidyverse)
sigma <- rexp(1, 1)
alpha <- rnorm(1,0,1)
b_x1 <- rnorm(1,10,0.3)
b_x2 <- rnorm(1, -2, 0.3)
x1 <- seq(from=0,to=5,length.out=30)
x2 <- seq(from=10,to=0,length.out=30)
df <- data.frame(
        x1=x1,x2=x2, sigma=sigma
        ) |> 
        mutate(mu=alpha*b_x1*x1+b_x2*x2)
df$y <- apply(df, 1, function(x) rnorm(1, mean=x[3], sd=x[4]))
df |> 
        pivot_longer(cols=c(x1,x2)) |> 
        ggplot(aes(x=value, y=y))+
        geom_point()+facet_wrap(~name)


```

### 5M2. v

Invent your own example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another.

```         
y=a1+b_x1*x1+b_x2*x2 
b_x1 = 1
b_x2 = -1
x1 = a2 +b2_x2*x2
b2_x2 = 1
```

### 5M3. v

It is sometimes observed that the best predictor of fire risk is the presence of firefighters— States and localities with many firefighters also have more fires. Presumably firefighters do not cause fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the same reversal of causal inference in the context of the divorce and marriage data. How might a high divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using multiple regression?

```         
a high divorce will allow for people people to marry several times so while I'm expecting a correlation between marriage rate and divorce rate, I'm expecting that correlation to dissapear when we control for median age of marriage
```

```{r}
data("WaffleDivorce")
d <- WaffleDivorce
d$M <- standardize(d$Marriage)
d$D <- standardize(d$Divorce)
d$A <- standardize(d$MedianAgeMarriage)
m5m3a <- quap(alist(
        D ~  dnorm(mu, sigma),
        mu <-  a+bM*M,
        a ~ dnorm(0,0.2),
        bM ~ dnorm(0,0.5),
        sigma ~ dexp(1)
),data=d)
m5m3b <- quap(alist(
        D~dnorm(mu, sigma),
        mu <-  a+bM*M+bA*A,
        a ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        bA ~ dnorm(0,0.5),
        sigma ~ dexp(1)
),data=d)
plot(coeftab(m5m3a,m5m3b), pars=c("bA","bM"))
```

### 5M4. v

In the divorce data, States with high numbers of members of the Church of Jesus Christ of Latter-day Saints (LDS) have much lower divorce rates than the regression models expected. Find a list of LDS population by State and use those numbers as a predictor variable, predicting divorce rate using marriage rate, median age at marriage, and percent LDS population (possibly standardized). You may want to consider transformations of the raw percent LDS variable.

```{r}
data("WaffleDivorce")
d <- WaffleDivorce
d$pct_LDS <- c(0.75, 4.53, 6.18, 1, 2.01, 2.82, 0.43, 0.55, 0.38,
0.75, 0.82, 5.18, 26.35, 0.44, 0.66, 0.87, 1.25, 0.77, 0.64, 0.81,
0.72, 0.39, 0.44, 0.58, 0.72, 1.14, 4.78, 1.29, 0.61, 0.37, 3.34,
0.41, 0.82, 1.48, 0.52, 1.2, 3.85, 0.4, 0.37, 0.83, 1.27, 0.75,
1.21, 67.97, 0.74, 1.13, 3.99, 0.92, 0.44, 11.5 )
d$L <- standardize( d$pct_LDS )
d$A <- standardize( d$MedianAgeMarriage )
d$M <- standardize( d$Marriage )
d$D <- standardize( d$Divorce )

m_5M4 <- quap(
        alist(
                D ~ dnorm(mu,sigma),
                mu <- a + bM*M + bA*A + bL*L,
                a ~ dnorm(0,0.2),
                c(bA,bM,bL) ~ dnorm(0,0.5),
                sigma ~ dexp(1)
                ), data=d )
precis( m_5M4 )
```

### 5M5.

One way to reason through multiple causation hypotheses is to imagine detailed mechanisms through which predictor variables may influence outcomes. For example, it is sometimes argued that the price of gasoline (predictor variable) is positively associated with lower obesity rates (outcome variable). However, there are at least two important mechanisms by which the price of gas could reduce obesity. First, it could lead to less driving and therefore more exercise. Second, it could lead to less driving, which leads to less eating out, which leads to less consumption of huge restaurant meals. Can you outline one or more multiple regressions that address these two mechanisms? Assume you can have any predictor data you need.

```         
```

## Hard

### 5H1. v

In the divorce example, suppose the DAG is: M → A → D. What are the implied conditional independencies of the graph? Are the data consistent with it?

```{r}
library(dagitty)
dag_5H1 <- dagitty("dag{M->A->D}")
impliedConditionalIndependencies(dag_5H1)
```

```{r}
m_5h1 <- quap(
        alist(
                D ~ dnorm(mu,sigma),
                mu <- a + bM*M + bA*A,
                a ~ dnorm(0,0.2),
                bM ~ dnorm(0,0.5),
                bA ~ dnorm(0,0.5),
                sigma ~ dexp(1)
                ), data=d )
precis( m_5h1 )
```

### 5H2. v

Assuming that the DAG for the divorce example is indeed M → A → D, fit a new model and use it to estimate the counterfactual effect of halving a State’s marriage rate M. Use the counterfactual example from the chapter (starting on page 140) as a template.

```{r}

m_5h2 <- quap(
        alist(
                D ~ dnorm(mu_D,sigma),
                mu_D <- aD +  bA*A,
                A ~ dnorm(mu_A, sigma),
                mu_A <- aA + bM*M,
                c(aD,aA) ~ dnorm(0,0.2),
                bM ~ dnorm(0,0.5),
                bA ~ dnorm(0,0.5),
                sigma ~ dexp(1)
                ), data=d )
precis( m_5h2 )
```

```{r}
mean_M <- mean(d$Marriage)
sd_M <- sd(d$Marriage)
sim2_dat <- data.frame( M=(c(20.2,10.1)-mean_M)/sd_M )
s2 <- sim( m_5h2 , data=sim2_dat , vars=c("A","D") )
mean( s2$D[,2] - s2$D[,1] )
```

### 5H3. v

Return to the milk energy model, m5.7. Suppose that the true causal relationship among the variables is:

```{r}
library(dagitty)
dag5h3 <- dagitty("dag{M -> K; M -> N; N -> K }")
coordinates(dag5h3) <- list(x=c(M=0, K=1, N=2),y=c(M=0, K=1, N=0))
drawdag(dag5h3)
```

Now compute the counterfactual effect on K of doubling M. You will need to account for both the direct and indirect paths of causation. Use the counterfactual example from the chapter (starting on page 140) as a template.

```{r}
m_5h3 <- quap(
        alist(
        K ~ dnorm(mu, sigma),
        mu <- a +bM*M+bN*N, 
        N ~ dnorm(mu_N, sigma),
        mu_N <- aN + bMN*M,
        a ~ dnorm(0,0.2),
        aN ~ dnorm(0,0.2),
        bN ~ dnorm(0,0.5),
        bM ~ dnorm(0,0.5),
        bMN ~ dnorm(0,0.5),
        sigma ~ dexp(1)
        ), data=dcc
)
mean_M <- mean(dcc$mass)
sd_M <- sd(dcc$mass)
sim2_dat_n <- data.frame( M=(dcc$mass-mean_M)/sd_M )
sim2_dat_d <- data.frame( M=(2*dcc$mass-mean_M)/sd_M )
s2_n <- sim( m_5h3 , data=sim2_dat_n , vars=c("K","N") )
s2_d <- sim( m_5h3 , data=sim2_dat_d , vars=c("K","N") )

data.frame(n=s2_n$K,d=s2_d$K) |> 
        ggplot(aes(x=n,y=d))+geom_point()
```

### 5H4.

Here is an open practice problem to engage your imagination. In the divorce date, States in the southern United States have many of the highest divorce rates. Add the South indicator variable to the analysis. First, draw one or more DAGs that represent your ideas for how Southern American culture might influence any of the other three variables (D, M or A). Then list the testable implications of your DAGs, if there are any, and fit one or more models to evaluate the implications. What do you think the influence of “Southerness” is ?

## Extra. v

Lectures A04 and A05 focus on estimating total and direct effects of sex on weight in adults from the Howell1 data. Now I want you to focus on the portion of the sample that is 13 years old and younger. Repeat the analysis, estimating both the total effect of sex on weight and the direct effect. What if anything is different about this subsample, compared to adults?

```{r}
library(rethinking)
data("Howell1")
d <- Howell1[Howell1$age<14,]
d$H <- d$height
d$W <- d$weight
d$sex <- d$male+1
```

```{r}
m_a05_x <- quap(alist(
        # H <- f(W,age,sex)
        W ~ dnorm(mu, sigma),
        mu <-   bH[sex]*H ,
        #a[sex] ~ dnorm(0.5, 0.5),
        bH[sex] ~ dnorm(0.5, 0.2),
        sigma ~ dexp(1),
        # W <- f(age, sex)
        H ~ dnorm(mu_H, sigma_H),
        mu_H <- a_H[sex]+  bHA[sex]*age,
        a_H[sex] ~ dnorm(60, 5),
        bHA[sex] ~ dnorm(0.5, 0.2),
        sigma_H ~ dexp(1)
), data=d)

#plot(coeftab(,, depth = 2,pars=c("bH","bA","bHA")))

plot(precis(m_a05_x, depth = 2))
# Seems like most of the influence of height in weigth it's caused by age. Any of the models where age is controled, bH is very close to zero. sigma_H is bigger than what i would expect. It seems there's a significant difference between birth height in girls and boys but not in height increase per year

```
